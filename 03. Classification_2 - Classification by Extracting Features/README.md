<h1 align="center"> Classification 02 ‚Äî Classification by Extracting Features  </h1>

### 1Ô∏è‚É£ Goal
- ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶°‡ßá‡¶ü‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶® (Feature Extraction) ‡¶∂‡ßá‡¶ñ‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßá‡¶ü‡¶ø‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ‡•§
- ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü‡ßá‡¶õ‡ßá BBC news dataset, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶®‡¶ø‡¶â‡¶ú ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶Ü‡¶õ‡ßá ‡¶Ø‡ßá‡¶Æ‡¶® ‚Äî business, politics, sports, tech, entertainment ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø‡•§

### 2Ô∏è‚É£ Importing Necessary Libraries
- ‡¶∏‡¶¨ ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø :
    ```python
    # ‡¶°‡ßá‡¶ü‡¶æ ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶≤‡¶ø‡¶Ç ‡¶ì ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
    import pandas as pdimport pandas as pd
    import numpy as 
    
    # ‡¶°‡ßá‡¶ü‡¶æ ‡¶≠‡¶ø‡¶ú‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
    import matplotlib.pyplot as plt
    import seaborn as 
    
    # ‡¶°‡ßá‡¶ü‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶ì ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
    from sklearn.model_selection import train_test_split

    # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

    # ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶Æ‡¶°‡ßá‡¶≤
    from sklearn.linear_model import LogisticRegression
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.svm import LinearSVC

    # ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶á‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ‡¶Ø‡¶º‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
    from sklearn.metrics import classification_report, confusion_matrix
    ```

### 3Ô∏è‚É£ Importing a Dataset
- Google Drive ‡¶Æ‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá: 
  ```python
  from google.colab import drive
  drive.mount('/content/drive') # ‡¶è‡¶ü‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü ‡¶Ø‡¶æ‡¶§‡ßá ‡¶ó‡ßÅ‡¶ó‡¶≤ ‡¶°‡ßç‡¶∞‡¶æ‡¶á‡¶≠ ‡¶•‡ßá‡¶ï‡ßá ‡¶´‡¶æ‡¶á‡¶≤ (‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü) ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü‡•§
  ```
- Excel file (BBC ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü) ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá:
  ```python
  df = pd.read_excel("/content/drive/MyDrive/path_to_your_file/bbc_dataset.xlsx")
  df.head()
  ```
### 4Ô∏è‚É£ Exploring the Dataset & Class Distribution
- ‡¶Æ‡ßã‡¶ü ‡¶ï‡¶§‡¶ó‡ßÅ‡¶≤‡ßã ‡¶°‡ßá‡¶ü‡¶æ ‡¶Ü‡¶õ‡ßá ‡¶è‡¶¨‡¶Ç ‡¶ï‡ßü‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶Ü‡¶õ‡ßá ‡¶§‡¶æ ‡¶™‡ßç‡¶∞‡¶ø‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§
- sns.countplot() ‡¶¶‡¶ø‡ßü‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá ‡¶ï‡¶§‡¶ü‡¶ø ‡¶°‡ßá‡¶ü‡¶æ ‡¶Ü‡¶õ‡ßá ‡¶§‡¶æ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã ‡¶π‡ßü (‡¶Ø‡ßá‡¶Æ‡¶® sports, politics ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø)‡•§
  ```python
    print("Number of samples:", len(df))          # ‡¶Æ‡ßã‡¶ü ‡¶ï‡¶§‡¶ó‡ßÅ‡¶≤‡ßã ‡¶∞‡ßã ‡¶Ü‡¶õ‡ßá
    print("Classes:", df['label'].unique())       # ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¶‡ßá‡¶ñ‡¶æ / ‡¶Æ‡ßã‡¶ü ‡¶ï‡ßü‡¶ü‡¶æ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶Ü‡¶õ‡ßá

    # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá ‡¶ï‡ßü‡¶ü‡¶æ ‡¶°‡ßá‡¶ü‡¶æ ‡¶Ü‡¶õ‡ßá ‡¶§‡¶æ‡¶∞ ‡¶ó‡ßç‡¶∞‡¶æ‡¶´
    sns.countplot(x="label", data=df)             
    plt.title("Class Distribution")               # ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤ ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ
    plt.show()                                    # ‡¶ó‡ßç‡¶∞‡¶æ‡¶´ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã


### 5Ô∏è‚É£ Text Cleaning (Preprocessing)
- nltk stopwords ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Ö‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶¨‡¶æ‡¶¶ ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡ßü, ‡¶Ø‡ßá‡¶Æ‡¶® ‚Äî "is", "the", "and" ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø‡•§
- re.sub() ‡¶¶‡¶ø‡ßü‡ßá ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶ö‡¶ø‡¶π‡ßç‡¶®, ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø ‡¶¨‡¶æ‡¶¶ ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡ßü‡•§
- ‡¶∏‡¶¨ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡¶ï‡ßá ‡¶õ‡ßã‡¶ü ‡¶π‡¶æ‡¶§‡ßá‡¶∞ ‡¶Ö‡¶ï‡ßç‡¶∑‡¶∞‡ßá (lowercase) ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§
  ```python 
    # ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø ‡¶á‡¶Æ‡¶™‡ßã‡¶∞‡ßç‡¶ü:
    import re                                      # ‡¶∞‡ßá‡¶ó‡ßÅ‡¶≤‡¶æ‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏‡¶™‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶™‡¶∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá
    from nltk.corpus import stopwords              # ‡¶Ö‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶¨‡¶æ‡¶¶ ‡¶¶‡¶ø‡¶§‡ßá
    import nltk                                    # Natural Language Toolkit
    nltk.download("stopwords")                     # stopwords ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ

    stop_words = set(stopwords.words("english"))   # ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø stopwords ‡¶∏‡ßá‡¶ü‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ

    # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶™‡¶∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®
    def clean_text(text):
        text = text.lower()                                  # ‡¶∏‡¶¨ ‡¶Ö‡¶ï‡ßç‡¶∑‡¶∞ ‡¶õ‡ßã‡¶ü ‡¶π‡¶æ‡¶§‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
        text = re.sub(r"[^a-z\\s]", "", text)                # ‡¶Ö‡¶ï‡ßç‡¶∑‡¶∞ ‡¶õ‡¶æ‡ßú‡¶æ ‡¶∏‡¶¨ ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¨‡¶æ‡¶¶ ‡¶¶‡ßá‡¶ì‡ßü‡¶æ
        return " ".join([w for w in text.split() if w not in stop_words])  # stopword (‡¶Ö‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü ‡¶∂‡¶¨‡ßç‡¶¶) ‡¶¨‡¶æ‡¶¶ ‡¶¶‡ßá‡¶Ø‡¶º‡¶æ 

    # ‡¶°‡ßá‡¶ü‡¶æ‡¶´‡ßç‡¶∞‡ßá‡¶Æ‡ßá ‡¶®‡¶§‡ßÅ‡¶® ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶ï‡ßç‡¶≤‡¶ø‡¶® ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶•‡¶æ‡¶ï‡¶¨‡ßá
    df["clean_text"] = df["text"].apply(clean_text)
    ```

### 6Ô∏è‚É£ Clean Data Preview
- ‡¶ï‡ßç‡¶≤‡¶ø‡¶® ‡¶ï‡¶∞‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶∞ ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶¶‡ßá‡¶ñ‡¶æ:
  ```python
    df.head()    # ‡¶è‡¶ñ‡¶® ‚Äúclean_text‚Äù ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶Ü‡¶õ‡ßá
    df[["text", "clean_text"]].head()  # ‡¶Ü‡¶∏‡¶≤ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶¨‡¶Ç ‡¶ï‡ßç‡¶≤‡¶ø‡¶® ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡ß´‡¶ü‡¶ø ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã
    ```
### 7Ô∏è‚É£ Feature Extraction (Keyword Feature Extraction)
- Keyword Feature Extraction ‡¶π‡¶≤‡ßã ‡¶è‡¶ï ‡¶ß‡¶∞‡¶£‡ßá‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßÅ‡ßü‡¶æ‡¶≤ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶á‡¶û‡ßç‡¶ú‡¶ø‡¶®‡¶ø‡ßü‡¶æ‡¶∞‡¶ø‡¶Ç,
‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶∞ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∂‡¶¨‡ßç‡¶¶ (keywords) ‡¶¨‡ßá‡¶õ‡ßá ‡¶®‡¶ø‡ßü‡ßá
‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶´‡ßç‡¶∞‡¶ø‡¶ï‡ßã‡ßü‡ßá‡¶®‡ßç‡¶∏‡¶ø (‡¶¨‡¶æ‡¶∞‡¶¨‡¶æ‡¶∞ ‡¶â‡¶™‡¶∏‡ßç‡¶•‡¶ø‡¶§‡¶ø) ‡¶ó‡ßÅ‡¶£‡¶ø‡•§

    ```python
        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶ï‡ßÄ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°‡ßá‡¶∞ ‡¶§‡¶æ‡¶≤‡¶ø‡¶ï‡¶æ
        keywords = [
        # Business
        "market", "stock", "bank", "economy", "finance", "trade", "growth",
        # Politics
        "election", "government", "minister", "policy", "parliament", "law",
        # Sport
        "football", "cricket", "tennis", "match", "team", "goal", "tournament",
        # Tech
        "technology", "computer", "internet", "software", "ai", "digital", "innovation",
        # Entertainment
            "film", "music", "movie", "actor", "actress", "award", "theatre"
        ]

        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá ‡¶è‡¶á ‡¶ï‡ßÄ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ï‡¶§‡¶¨‡¶æ‡¶∞ ‡¶è‡¶∏‡ßá‡¶õ‡ßá ‡¶§‡¶æ ‡¶ó‡ßÅ‡¶®‡ßá ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá‡•§
        def extract_keyword_features(texts, keywords):
        features = []                             # ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶∞‡¶æ‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ñ‡¶æ‡¶≤‡¶ø ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü
        for text in texts:                        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
            words = text.split()                  # ‡¶∂‡¶¨‡ßç‡¶¶‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ
            counts = [words.count(kw) for kw in keywords]  # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßÄ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶° ‡¶ï‡ßü‡¶¨‡¶æ‡¶∞ ‡¶è‡¶∏‡ßá‡¶õ‡ßá ‡¶§‡¶æ ‡¶ó‡¶£‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ
            features.append(counts)               # ‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ
        return np.array(features)                 # NumPy ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∞‡ßá ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡¶æ

        # ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ì ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶∏‡ßá‡¶ü‡ßá ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶® ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã
        X_train_keywords = extract_keyword_features(X_train, keywords)
        X_test_keywords = extract_keyword_features(X_test, keywords)

        print("Keyword feature shape:", X_train_keywords.   shape)  # ‡¶Ü‡¶ï‡¶æ‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶æ (‡¶®‡¶Æ‡ßÅ‡¶®‡¶æ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ √ó ‡¶ï‡ßÄ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶° ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ)
    ```

### ‡¶è‡¶ñ‡¶æ‡¶® ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶°‡ßá‡¶ü‡¶æ ‡¶≤‡ßã‡¶° ‚Üí ‡¶ï‡ßç‡¶≤‡¶ø‡¶®‡¶ø‡¶Ç ‚Üí ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶® ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§

### 8Ô∏è‚É£ Keyword Frequency by Label
- ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ (‡¶Ø‡ßá‡¶Æ‡¶® ‚Äî business, politics, sport) ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ
 ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ keywords ‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶´‡ßç‡¶∞‡¶ø‡¶ï‡ßã‡ßü‡ßá‡¶®‡ßç‡¶∏‡¶ø ‡¶ó‡¶£‡¶®‡¶æ ‡¶ï‡¶∞‡ßá ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤ ‡¶Ü‡¶ï‡¶æ‡¶∞‡ßá ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã‡•§
  ```python
    # ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø:
    from collections import Counter

    # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶á‡¶â‡¶®‡¶ø‡¶ï label (‡¶Ø‡ßá‡¶Æ‡¶® ‚Äî business, sports, politics ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø) ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶æ Counter() ‡¶Ö‡¶¨‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§ 
    # ‡¶è‡¶á Counter ‡¶¶‡¶ø‡ßü‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ê ‡¶≤‡ßá‡¶¨‡ßá‡¶≤‡ßá‡¶∞ ‡¶Ö‡¶ß‡ßÄ‡¶®‡ßá ‡¶•‡¶æ‡¶ï‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶´‡ßç‡¶∞‡¶ø‡¶ï‡ßã‡ßü‡ßá‡¶®‡ßç‡¶∏‡¶ø ‡¶∞‡¶æ‡¶ñ‡¶¨‡•§
    def keyword_frequency_by_label(df, keywords):
        label_keyword_counts = {label: Counter() for label in df["label"].unique()}
        
    # Count keyword occurrences per label : iterrows() ‡¶¶‡¶ø‡ßü‡ßá DataFrame-‡¶è‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∞‡ßã ‡¶ò‡ßÅ‡¶∞‡ßá ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá‡•§ clean_text ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ó‡ßã‡¶®‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá‡•§ ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ ‡¶Ø‡¶¶‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶‡¶ü‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ keywords list-‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶¨‡ßá ‡¶∏‡ßá‡¶á ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ count ‡¶ê ‡¶≤‡ßá‡¶¨‡ßá‡¶≤‡ßá‡¶∞ Counter-‡¶è ‡¶Ø‡ßã‡¶ó ‡¶π‡¶ö‡ßç‡¶õ‡ßá‡•§
        for _, row in df.iterrows():
            words = row["clean_text"].split()
            counts = Counter(words)
            for kw in keywords:
                if kw in counts:
                    label_keyword_counts[row["label"]][kw] += counts[kw]

    # Convert to DataFrame : ‡¶∏‡¶¨ Counter dictionary-‡¶ï‡ßá ‡¶è‡¶ï‡¶§‡ßç‡¶∞‡ßá pandas DataFrame-‡¶è ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§ .fillna(0) ‚Üí ‡¶Ø‡¶¶‡¶ø ‡¶ï‡ßã‡¶®‡ßã ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶ï‡ßã‡¶®‡ßã ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶§‡ßá ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá 0 ‡¶¨‡¶∏‡¶æ‡¶®‡ßã ‡¶π‡ßü‡•§ .astype(int) ‚Üí ‡¶∏‡¶¨ ‡¶Æ‡¶æ‡¶® integer-‡¶è ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶ø‡¶§ ‡¶π‡ßü‡•§
        freq_df = pd.DataFrame(label_keyword_counts).fillna(0).astype(int)
        return freq_df

    # Run on your dataset : df (BBC ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü) ‡¶ì keywords ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡ßá
    freq_df = keyword_frequency_by_label(df, keywords)

    # Show top keyword frequencies per label : DataFrame-‡¶è‡¶∞ index (‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡ßé ‡¶∂‡¶¨‡ßç‡¶¶) ‡¶ó‡ßÅ‡¶≤‡ßã alphabetically sort ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá‡•§ ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ ‡¶™‡ßç‡¶∞‡¶ø‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡ßá ‡¶¶‡ßá‡¶ñ‡¶æ‡¶ö‡ßç‡¶õ‡ßá, ‡¶ï‡ßã‡¶® ‡¶≤‡ßá‡¶¨‡ßá‡¶≤‡ßá ‡¶ï‡ßã‡¶® ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶ï‡¶§‡¶¨‡¶æ‡¶∞ ‡¶è‡¶∏‡ßá‡¶õ‡ßá‡•§
    print(freq_df.sort_index())
  ```
### 9Ô∏è‚É£ Keyword Frequency Visualization (Heatmap + Bar Chart)
- sns.heatmap() ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßÄ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶° ‡¶¨‡¶®‡¶æ‡¶Æ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶≤‡ßá‡¶¨‡ßá‡¶≤‡ßá‡¶∞ ‡¶´‡ßç‡¶∞‡¶ø‡¶ï‡ßã‡ßü‡ßá‡¶®‡ßç‡¶∏‡¶ø ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤‡¶ï‡ßá ‡¶∞‡¶ô‡¶ø‡¶®‡¶≠‡¶æ‡¶¨‡ßá ‡¶¶‡ßá‡¶ñ‡¶æ‡ßü‡•§ ‡¶Ø‡ßá ‡¶ú‡¶æ‡ßü‡¶ó‡¶æ‡ßü ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∂‡¶ø, ‡¶∏‡ßá‡¶ü‡¶ø ‡¶ó‡¶æ‡ßù ‡¶∞‡¶ô‡ßá ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá‡•§
  ```python 
    # üî• Heatmap of keyword frequencies
    plt.figure(figsize=(12,8))                                # ‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∞ ‡¶∏‡¶æ‡¶á‡¶ú ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ
    sns.heatmap(freq_df, annot=True, fmt="d", cmap="YlGnBu")  # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∏‡ßá‡¶≤‡ßá ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶∏‡¶π ‡¶π‡¶ø‡¶ü‡¶Æ‡ßç‡¶Ø‡¶æ‡¶™ ‡¶Ü‡¶Å‡¶ï‡¶æ
    plt.title("Keyword Frequency per Label")                  # ‡¶ó‡ßç‡¶∞‡¶æ‡¶´‡ßá‡¶∞ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤
    plt.xlabel("News Label")                                  # ‡¶è‡¶ï‡ßç‡¶∏-‡¶Ö‡¶ï‡ßç‡¶∑‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ
    plt.ylabel("Keyword")                                     # ‡¶ì‡ßü‡¶æ‡¶á-‡¶Ö‡¶ï‡ßç‡¶∑‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ
    plt.show()                                                # ‡¶ó‡ßç‡¶∞‡¶æ‡¶´ ‡¶™‡ßç‡¶∞‡¶¶‡¶∞‡ßç‡¶∂‡¶®
  ``` 
- ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶≤‡ßá‡¶¨‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶¨‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶∞‡ßç‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡ßü‡•§ ‡¶è‡¶§‡ßá ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡ßü ‡¶ï‡ßã‡¶® ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ê ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶∞ ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡•§
  ```python 
    # üìä Bar chart: Top keywords per label
    for label in df["label"].unique():                   # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶≤‡ßá‡¶¨‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶≤‡ßÅ‡¶™ ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã
        top_keywords = freq_df[label].sort_values(ascending=False).head(10)  # ‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑ ‡ßß‡ß¶‡¶ü‡¶ø ‡¶ï‡ßÄ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶° ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®
        plt.figure(figsize=(8,4))                        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ö‡¶æ‡¶∞‡ßç‡¶ü‡ßá‡¶∞ ‡¶∏‡¶æ‡¶á‡¶ú ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£
        sns.barplot(x=top_keywords.values, y=top_keywords.index, palette="viridis")  # ‡¶¨‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶∞‡ßç‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
        plt.title(f"Top Keywords in {label} Articles")   # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶≤‡ßá‡¶¨‡ßá‡¶≤‡ßá‡¶∞ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤
        plt.xlabel("Frequency")                          # ‡¶è‡¶ï‡ßç‡¶∏-‡¶Ö‡¶ï‡ßç‡¶∑‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ
        plt.ylabel("Keyword")                            # ‡¶ì‡ßü‡¶æ‡¶á-‡¶Ö‡¶ï‡ßç‡¶∑‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ
        plt.show()                                       # ‡¶ö‡¶æ‡¶∞‡ßç‡¶ü ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã
  ```

### 1Ô∏è‚É£0Ô∏è‚É£ Combine BoW + TF-IDF + Keyword Features (Hybrid Feature Extraction)
- BoW (Bag of Words) : ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶â‡¶™‡¶∏‡ßç‡¶•‡¶ø‡¶§‡¶ø‡¶∞ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ (frequency) ‡¶ß‡¶∞‡ßá ‡¶∞‡¶æ‡¶ñ‡ßá‡•§
- TF-IDF : ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶ì‡¶ú‡¶® (weight) ‡¶¨‡ßá‡¶∂‡¶ø ‡¶¶‡ßá‡ßü, ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶ì‡¶ú‡¶® ‡¶ï‡¶Æ‡¶æ‡ßü‡•§
- Keyword Features : ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßÅ‡ßü‡¶æ‡¶≤‡¶ø ‡¶¨‡¶æ‡¶õ‡¶æ‡¶á ‡¶ï‡¶∞‡¶æ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá ‡¶ß‡¶∞‡ßá ‡¶∞‡¶æ‡¶ñ‡ßá‡•§
- hstack() : ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏‡¶ï‡ßá ‡¶™‡¶æ‡¶∂‡ßá ‡¶™‡¶æ‡¶∂‡ßá ‡¶ú‡ßã‡ßú‡¶æ ‡¶≤‡¶æ‡¶ó‡¶æ‡ßü (horizontal stack)‡•§
- ‡¶è‡¶ñ‡¶® ‡¶è‡¶ï‡¶ü‡¶ø ‡¶π‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡¶ø‡¶° ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶∏‡ßá‡¶ü ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶ó‡ßá‡¶õ‡ßá ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø **‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡ßá = BoW + TF-IDF + Keyword** ‡¶§‡¶•‡ßç‡¶Ø ‡¶è‡¶ï‡¶§‡ßç‡¶∞‡ßá ‡¶Ü‡¶õ‡ßá‡•§
‡¶è‡¶ü‡¶ø ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ **accuracy** ‡¶¨‡¶æ‡ßú‡¶æ‡¶§‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡¶Æ‡ßü ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡ßá ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶è‡¶§‡ßá
**statistical + semantic + domain** ‡¶§‡¶•‡ßç‡¶Ø ‡¶∏‡¶¨ ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡•§

  ```python 
    from scipy.sparse import hstack   # sparse ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏‡¶ó‡ßÅ‡¶≤‡ßã ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø

    # Define vectorizers
    bow_vectorizer = CountVectorizer(max_features=5000)   # Bag of Words ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ (‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß´‡ß¶‡ß¶‡ß¶ ‡¶∂‡¶¨‡ßç‡¶¶)
    tfidf_vectorizer = TfidfVectorizer(max_features=5000) # TF-IDF ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ (‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß´‡ß¶‡ß¶‡ß¶ ‡¶∂‡¶¨‡ßç‡¶¶)

    # Fit and transform BoW
    X_train_bow = bow_vectorizer.fit_transform(X_train)   # ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶°‡ßá‡¶ü‡¶æ ‡¶•‡ßá‡¶ï‡ßá BoW ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶∂‡ßá‡¶ñ‡¶æ
    X_test_bow = bow_vectorizer.transform(X_test)         # ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡ßá‡¶ü‡¶æ ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡¶æ

    # Fit and transform TF-IDF
    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)  # ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶°‡ßá‡¶ü‡¶æ ‡¶•‡ßá‡¶ï‡ßá TF-IDF ‡¶∂‡ßá‡¶ñ‡¶æ
    X_test_tfidf = tfidf_vectorizer.transform(X_test)        # ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡ßá‡¶ü‡¶æ ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡¶æ

    # Combine all features together
    X_train_combined = hstack([X_train_bow, X_train_tfidf, X_train_keywords])  # ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶è‡¶ï‡¶§‡ßç‡¶∞‡ßá
    X_test_combined = hstack([X_test_bow, X_test_tfidf, X_test_keywords])

    print("‚úÖ Final combined feature shape:", X_train_combined.shape)
  ```
### 1Ô∏è‚É£1Ô∏è‚É£ Train & Evaluate Multiple Models with Hybrid 
- Models dictionary: ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá‡•§
- Training: ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶Æ‡¶°‡ßá‡¶≤‡¶ï‡ßá X_train_combined ‡¶è‡¶¨‡¶Ç y_train ‡¶¶‡¶ø‡ßü‡ßá ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá‡•§
- Prediction: ‡¶π‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡¶ø‡¶° ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞‡ßá‡¶∞ ‡¶ì‡¶™‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø ‡¶ï‡¶∞‡ßá ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡ßá‡¶ü‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶®‡•§
- Evaluation: 
  - classification_report() ‚Üí precision, recall, f1-score, support
  - confusion_matrix() ‚Üí ‡¶ï‡ßã‡¶® ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶ï‡¶§‡¶ü‡¶æ ‡¶∏‡¶†‡¶ø‡¶ï ‡¶¨‡¶æ ‡¶≠‡ßÅ‡¶≤ ‡¶π‡ßü‡ßá‡¶õ‡ßá
- Visualization: ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø heatmap ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã ‡¶π‡¶ö‡ßç‡¶õ‡ßá, ‡¶Ø‡¶æ‡¶§‡ßá ‡¶∏‡¶π‡¶ú‡ßá ‡¶™‡¶æ‡¶∞‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶∏ ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü‡•§
  ```python 
    # Import models
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.svm import LinearSVC
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier

    # Define models dictionary
    models = {
        "Logistic Regression": LogisticRegression(max_iter=1000),
        "Naive Bayes": MultinomialNB(),
        "Linear SVM": LinearSVC(),
        "Decision Tree": DecisionTreeClassifier(),
        "Random Forest": RandomForestClassifier(n_estimators=100)
    }

    # Dictionary to store results
    results = {}

    # Train, predict & evaluate each model
    for name, model in models.items():
        print(f"\n Training {name}...")                  # ‡¶ï‡ßã‡¶® ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶§‡¶æ ‡¶™‡ßç‡¶∞‡¶ø‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ
        model.fit(X_train_combined, y_train)             # ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶´‡¶ø‡¶ü/‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ï‡¶∞‡¶æ
        preds = model.predict(X_test_combined)           # ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡ßá‡¶ü‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶® ‡¶ï‡¶∞‡¶æ

        # Print classification report
        print(f"--- {name} Report ---")
        print(classification_report(y_test, preds))      # precision, recall, f1-score ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã

        # Store results
        results[name] = classification_report(y_test, preds, output_dict=True)

        # Confusion Matrix
        cm = confusion_matrix(y_test, preds, labels=model.classes_)
        sns.heatmap(cm, annot=True, fmt="d", xticklabels=model.classes_, yticklabels=model.classes_)
        plt.title(f"Confusion Matrix - {name}")
        plt.show()
  ```
### 1Ô∏è‚É£2Ô∏è‚É£ Accuracy Comparison Bar Chart
- results dictionary-‡¶§‡ßá ‡¶∏‡¶¨ ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ classification report ‡¶Ü‡¶õ‡ßá‡•§
- ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ accuracy ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§
- sns.barplot() ‡¶¶‡¶ø‡ßü‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶∞‡ßç‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø ‡¶Ø‡¶æ ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ ‡¶∏‡¶π‡¶ú ‡¶ï‡¶∞‡ßá‡•§
- X-‡¶Ö‡¶ï‡ßç‡¶∑ ‚Üí ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ
- Y-‡¶Ö‡¶ï‡ßç‡¶∑ ‚Üí Accuracy (%)
  ```python
    # Extract accuracy scores from results dictionary
    accuracy_scores = {name: results[name]["accuracy"] for name in results}

    # Plot bar chart
    plt.figure(figsize=(8,5))
    sns.barplot(x=list(accuracy_scores.keys()), y=list(accuracy_scores.values()))
    plt.title("Model Accuracy Comparison")
    plt.ylabel("Accuracy")
    plt.xticks(rotation=30)
    plt.show()
  ```

### 1Ô∏è‚É£3Ô∏è‚É£ Training Logistic Regression (Hybrid Features)
- Hybrid Features: ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶π‡¶ö‡ßç‡¶õ‡ßá BoW + TF-IDF + Keyword ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá‡•§
- max_iter=1000: ‡¶≤‡¶ú‡¶ø‡¶∏‡ßç‡¶ü‡¶ø‡¶ï ‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø iteration ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡•§
- classification_report(): ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá‡¶∞ precision, recall, f1-score ‡¶è‡¶¨‡¶Ç overall accuracy ‡¶¶‡ßá‡¶ñ‡¶æ‡ßü‡•§
  ```python 
    # ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø ‡¶á‡¶Æ‡¶™‡ßã‡¶∞‡ßç‡¶ü
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix
    import seaborn as sns
    import matplotlib.pyplot as plt

    # Initialize and train Logistic Regression model
    lr_model = LogisticRegression(max_iter=1000)     # ‡¶∏‡¶∞‡ßç‡¶¨‡¶æ‡¶ß‡¶ø‡¶ï 1000 iteration ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç

    print("\nTraining Logistic Regression...")
    lr_model.fit(X_train_combined, y_train)          # ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ ‡¶¶‡¶ø‡ßü‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶´‡¶ø‡¶ü/‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ï‡¶∞‡¶æ
    lr_preds = lr_model.predict(X_test_combined)     # ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡ßá‡¶ü‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶® ‡¶ï‡¶∞‡¶æ

    print("--- Logistic Regression Report ---")
    print(classification_report(y_test, lr_preds))   # precision, recall, f1-score ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã
  ```
### 1Ô∏è‚É£4Ô∏è‚É£ Testing : Predict Category for New Article
- Preprocessing: ‡¶®‡¶§‡ßÅ‡¶® ‡¶Ü‡¶∞‡ßç‡¶ü‡¶ø‡¶ï‡ßá‡¶≤‡¶ï‡ßá ‡¶ï‡ßç‡¶≤‡¶ø‡¶® ‡¶ï‡¶∞‡¶æ (‡¶õ‡ßã‡¶ü ‡¶π‡¶æ‡¶§‡ßá‡¶∞ ‡¶Ö‡¶ï‡ßç‡¶∑‡¶∞‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞, ‡¶∏‡ßç‡¶™‡ßá‡¶∂‡¶æ‡¶≤ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶æ‡¶∞ ‡¶ì stopwords ‡¶¨‡¶æ‡¶¶ ‡¶¶‡ßá‡¶ì‡ßü‡¶æ)
- Feature Extraction: ‡¶®‡¶§‡ßÅ‡¶® ‡¶Ü‡¶∞‡ßç‡¶ü‡¶ø‡¶ï‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø BoW, TF-IDF ‡¶è‡¶¨‡¶Ç Keyword ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
- Combine Features: hstack() ‡¶¶‡¶ø‡ßü‡ßá ‡¶∏‡¶¨ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶è‡¶ï‡¶§‡ßç‡¶∞‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ
- Prediction: ‡¶ü‡ßç‡¶∞‡ßá‡¶á‡¶®‡ßç‡¶° ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶®‡•§ prediction[0] ‡¶¶‡¶ø‡ßü‡ßá ‡¶ï‡ßá‡¶¨‡¶≤ ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡¶õ‡ßá‡•§ 
  ```python 
    # Predict Category for New Article
    def predict_news_category(article, model, bow_vectorizer, tfidf_vectorizer, keywords):
        import re
        import numpy as np

        # Preprocess text
        stop_words = set(stopwords.words("english"))     # stopwords ‡¶∏‡ßá‡¶ü ‡¶§‡ßà‡¶∞‡¶ø
        def clean_text(text):
            text = text.lower()                          # ‡¶õ‡ßã‡¶ü ‡¶π‡¶æ‡¶§‡ßá‡¶∞ ‡¶Ö‡¶ï‡ßç‡¶∑‡¶∞‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞
            text = re.sub(r"[^a-z\s]", "", text)         # ‡¶∏‡ßç‡¶™‡ßá‡¶∂‡¶æ‡¶≤ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶æ‡¶∞ ‡¶ì ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶¨‡¶æ‡¶¶ ‡¶¶‡ßá‡¶ì‡ßü‡¶æ    
            return " ".join([w for w in text.split() if w not in stop_words])             # stopwords ‡¶¨‡¶æ‡¶¶ ‡¶¶‡ßá‡¶ì‡ßü‡¶æ

        clean_article = clean_text(article)              # ‡¶Ü‡¶∞‡ßç‡¶ü‡¶ø‡¶ï‡ßá‡¶≤ ‡¶ï‡ßç‡¶≤‡¶ø‡¶® ‡¶ï‡¶∞‡¶æ

        # Feature extraction
        bow_feat = bow_vectorizer.transform([clean_article])      # BoW
        tfidf_feat = tfidf_vectorizer.transform([clean_article])  # TF-IDF

        # Keyword frequency
        def extract_keyword_features(texts, keywords):
            features = []
            for text in texts:
                words = text.split()
                counts = [words.count(kw) for kw in keywords]     # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßÄ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°‡ßá‡¶∞ ‡¶´‡ßç‡¶∞‡¶ø‡¶ï‡ßã‡ßü‡ßá‡¶®‡ßç‡¶∏‡¶ø ‡¶ó‡¶£‡¶®‡¶æ
                features.append(counts)
            return np.array(features)

        keyword_feat = extract_keyword_features([clean_article], keywords)

        # Combine features
        from scipy.sparse import hstack
        combined_feat = hstack([bow_feat, tfidf_feat, keyword_feat])

        # Predict
        prediction = model.predict(combined_feat)           # ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶®
        return prediction[0]                                # ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶®

    # Example: ‡¶®‡¶§‡ßÅ‡¶® ‡¶Ü‡¶∞‡ßç‡¶ü‡¶ø‡¶ï‡ßá‡¶≤ ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶®Example usage:
    new_article = "Bangladesh has made notable progress in digital financial transactions but remains far from becoming a cashless economy, experts said yesterday, calling for stronger policy support, improved infrastructure, and wider adoption to reduce reliance on physical currency."
    predicted_label = predict_news_category(new_article, lr_model, bow_vectorizer, tfidf_vectorizer, keywords)
    print(f"Predicted News Category: {predicted_label}")   # ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡ßç‡¶ü‡ßá‡¶° ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶™‡ßç‡¶∞‡¶ø‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ
    ```
